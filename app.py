import streamlit as st
import sys
import os
import asyncio
import re
from pathlib import Path

# === 1. Windows å¼‚æ­¥ç­–ç•¥è¡¥ä¸ ===
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# === 2. åŸºç¡€ç¯å¢ƒé…ç½® ===
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
sys.path.append(os.getcwd())

from sentence_transformers import SentenceTransformer
from lightrag.llm.openai import openai_complete_if_cache
from lightrag.utils import EmbeddingFunc
from raganything import RAGAnything, RAGAnythingConfig
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env", override=False)

st.set_page_config(page_title="æ–°å·¥ç§‘ AI åŠ©æ•™", layout="wide", page_icon="ğŸ“")

# === è§†è§‰æ¨¡å‹é…ç½® ===
# æ”¯æŒçš„è§†è§‰æ¨¡å‹æä¾›å•†é…ç½®
VISION_PROVIDERS = {
    "zhipu": {  # æ™ºè°± AI GLM-4V (æ¨è)
        "base_url": "https://open.bigmodel.cn/api/paas/v4/",
        "model": "glm-4v",
        "env_key": "ZHIPU_API_KEY"
    },
    "qwen": {  # é˜¿é‡Œé€šä¹‰åƒé—®
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "model": "qwen-vl-max",
        "env_key": "QWEN_API_KEY"
    },
    "siliconflow": {  # ç¡…åŸºæµåŠ¨
        "base_url": "https://api.siliconflow.cn/v1",
        "model": "Qwen/Qwen2-VL-72B-Instruct",
        "env_key": "SILICONFLOW_API_KEY"
    }
}

# === 3. æ°¸ä¹…äº‹ä»¶å¾ªç¯ç®¡ç† ===
if "loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    st.session_state.loop = loop
else:
    asyncio.set_event_loop(st.session_state.loop)

# === 4. è¾…åŠ©å‡½æ•°ï¼šæ¸…æ´—æ•°å­¦å…¬å¼æ ¼å¼ ===
def process_math_format(text):
    """
    å¤„ç† LLM è¿”å›çš„æ–‡æœ¬ä¸­çš„æ•°å­¦å…¬å¼ï¼Œç¡®ä¿èƒ½è¢« Streamlit æ­£ç¡®æ¸²æŸ“
    Streamlit æ”¯æŒ $...$ (è¡Œå†…) å’Œ $$...$$ (å—çº§) æ ¼å¼çš„ LaTeX
    """
    if not isinstance(text, str): return str(text)
    
    # 1. å°† \(...\) è½¬æ¢ä¸º $...$
    text = re.sub(r'\\\((.*?)\\\)', r'$\1$', text, flags=re.DOTALL)
    
    # 2. å°† \[...\] è½¬æ¢ä¸º $$...$$
    text = re.sub(r'\\\[(.*?)\\\]', r'$$\1$$', text, flags=re.DOTALL)
    
    # 3. å¤„ç†åå¼•å·ä¸­çš„å…¬å¼å†…å®¹
    def remove_code_ticks(match):
        content = match.group(1)
        if '\\' in content or '^' in content or '_' in content:
            return f"${content.strip('$')}$"
        return match.group(0)
    text = re.sub(r'`([^`]+)`', remove_code_ticks, text)
    
    # 4. æ ¸å¿ƒé€»è¾‘ï¼šæŒ‰è¡Œå¤„ç†ï¼Œæ‰¾åˆ°å…¬å¼æ®µè½å¹¶åŒ…è£¹
    def wrap_latex_in_line(line):
        """å¤„ç†å•è¡Œæ–‡æœ¬ï¼ŒåŒ…è£¹å…¶ä¸­çš„ LaTeX å…¬å¼"""
        # å¦‚æœè¡Œä¸­æ²¡æœ‰åæ–œæ æˆ–å·²ç»æœ‰ $ï¼Œè·³è¿‡
        if '\\' not in line:
            return line
        if line.strip().startswith('$') and line.strip().endswith('$'):
            return line
        
        # ä¸­æ–‡å­—ç¬¦èŒƒå›´
        def is_chinese(char):
            return '\u4e00' <= char <= '\u9fff' or char in 'ï¼Œã€‚ã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ï¼ï¼Ÿ'
        
        result = []
        i = 0
        n = len(line)
        
        while i < n:
            char = line[i]
            
            # å¦‚æœå·²ç»åœ¨ $ å†…ï¼Œç›´æ¥è·³è¿‡ç›´åˆ° $
            if char == '$':
                j = i + 1
                while j < n and line[j] != '$':
                    j += 1
                result.append(line[i:j+1] if j < n else line[i:])
                i = j + 1
                continue
            
            # æ£€æµ‹ LaTeX å…¬å¼å¼€å§‹
            if char == '\\' and i + 1 < n and line[i + 1].isalpha():
                latex_start = i
                
                # ä½¿ç”¨å¹³è¡¡æ‹¬å·æ³•æ‰¾åˆ°å…¬å¼ç»“æŸä½ç½®
                brace_depth = 0
                j = i
                last_valid_end = i
                
                while j < n:
                    c = line[j]
                    
                    # é‡åˆ°ä¸­æ–‡ï¼Œå…¬å¼ç»“æŸ
                    if is_chinese(c):
                        break
                    
                    if c == '{':
                        brace_depth += 1
                        j += 1
                        last_valid_end = j
                    elif c == '}':
                        brace_depth -= 1
                        j += 1
                        last_valid_end = j
                        # å¦‚æœæ‹¬å·å¹³è¡¡äº†ï¼Œæ£€æŸ¥åé¢æ˜¯å¦è¿˜æœ‰å…¬å¼å†…å®¹
                        if brace_depth == 0:
                            # è·³è¿‡ç©ºæ ¼
                            k = j
                            while k < n and line[k] == ' ':
                                k += 1
                            # æ£€æŸ¥åé¢æ˜¯å¦è¿˜æœ‰å…¬å¼ç›¸å…³å­—ç¬¦
                            if k < n and line[k] in '\\=+-^_{}':
                                j = k
                                continue
                            elif k < n and line[k] == '{':
                                # å¯èƒ½æ˜¯ \frac{}{} çš„ç¬¬äºŒä¸ªå‚æ•°
                                j = k
                                continue
                    elif c == '\\' and j + 1 < n and line[j + 1].isalpha():
                        # å¦ä¸€ä¸ª LaTeX å‘½ä»¤
                        j += 1
                        while j < n and (line[j].isalnum() or line[j] == '*'):
                            j += 1
                        last_valid_end = j
                    elif c in '^_':
                        j += 1
                        if j < n and line[j] == '{':
                            brace_depth += 1
                            j += 1
                        elif j < n and (line[j].isalnum() or line[j] == '\\'):
                            j += 1
                        last_valid_end = j
                    elif c.isalnum() or c in '.,+-=|<>()[]':
                        j += 1
                        last_valid_end = j
                    elif c == ' ':
                        # ç©ºæ ¼ï¼šæ£€æŸ¥åé¢æ˜¯å¦è¿˜æœ‰å…¬å¼å†…å®¹
                        k = j + 1
                        while k < n and line[k] == ' ':
                            k += 1
                        if k < n and (line[k] in '\\=+-^_{}' or line[k].isalnum()):
                            j = k
                        else:
                            break
                    else:
                        break
                
                # ç¡®ä¿æ‹¬å·å¹³è¡¡
                if brace_depth != 0:
                    j = last_valid_end
                
                latex_expr = line[latex_start:j].strip()
                
                # ç§»é™¤å°¾éƒ¨çš„æ ‡ç‚¹
                while latex_expr and latex_expr[-1] in 'ï¼Œã€‚ã€ï¼›ï¼š':
                    latex_expr = latex_expr[:-1]
                
                if latex_expr:
                    result.append(f'${latex_expr}$')
                
                i = j
            else:
                result.append(char)
                i += 1
        
        return ''.join(result)
    
    # æŒ‰è¡Œå¤„ç†
    lines = text.split('\n')
    processed_lines = [wrap_latex_in_line(line) for line in lines]
    text = '\n'.join(processed_lines)
    
    # 5. ä¿®å¤å¯èƒ½äº§ç”Ÿçš„é—®é¢˜
    # ä¿®å¤è¿ç»­çš„ $$ 
    text = re.sub(r'\$\$+', '$', text)  # å¤šä¸ª $ å˜æˆ 1 ä¸ª
    text = re.sub(r'\$\s*\$', '', text)  # ç§»é™¤ç©ºçš„ $$
    
    # 6. ä¿®å¤è¢«é”™è¯¯æ‹†åˆ†çš„å…¬å¼ï¼ˆå¦‚ $a^2$ = $b^2$ åº”è¯¥æ˜¯ $a^2 = b^2$ï¼‰
    def merge_adjacent_formulas(text):
        # åŒ¹é… $...$ç©ºæ ¼=ç©ºæ ¼$...$ è¿™æ ·çš„æ¨¡å¼å¹¶åˆå¹¶
        pattern = r'\$([^$]+)\$(\s*[=<>+\-]\s*)\$([^$]+)\$'
        while re.search(pattern, text):
            text = re.sub(pattern, r'$\1\2\3$', text)
        return text
    
    text = merge_adjacent_formulas(text)
    
    # 7. ç¡®ä¿ $...$ ä¹‹é—´æ²¡æœ‰æ¢è¡Œï¼ˆå¦åˆ™ Streamlit ä¸ä¼šæ¸²æŸ“ï¼‰
    def fix_multiline_inline_math(match):
        content = match.group(1)
        if '\n' in content:
            return f'$${content}$$'
        return match.group(0)
    
    text = re.sub(r'\$([^$]+)\$', fix_multiline_inline_math, text)
    
    return text

# === 5. æ¨¡å‹åŠ è½½ ===
@st.cache_resource
def load_local_model_only():
    print("æ­£åœ¨åŠ è½½æœ¬åœ° BGE-Small ä¸­æ–‡æ¨¡å‹...")
    return SentenceTransformer('BAAI/bge-small-zh-v1.5')

# === 6. æ ¸å¿ƒ RAG ä¸šåŠ¡é€»è¾‘ ===
async def run_rag(file_path, query, level):
    # DeepSeek é…ç½® (æ–‡æœ¬å¤„ç†)
    api_key = os.getenv("LLM_BINDING_API_KEY")
    base_url = os.getenv("LLM_BINDING_HOST")
    
    # è§†è§‰æ¨¡å‹é…ç½® (å›¾åƒå¤„ç†)
    vision_provider = os.getenv("VISION_PROVIDER", "zhipu")  # é»˜è®¤ä½¿ç”¨æ™ºè°±
    vision_config = VISION_PROVIDERS.get(vision_provider, VISION_PROVIDERS["zhipu"])
    vision_api_key = os.getenv(vision_config["env_key"]) or os.getenv("VISION_API_KEY")
    vision_base_url = os.getenv("VISION_BASE_URL") or vision_config["base_url"]
    vision_model = os.getenv("VISION_MODEL") or vision_config["model"]
    
    local_model = load_local_model_only()

    async def _current_loop_embed(texts):
        return await asyncio.to_thread(lambda: local_model.encode(texts))

    embedding_func = EmbeddingFunc(
        embedding_dim=512, 
        max_token_size=512, 
        func=_current_loop_embed
    )

    # æ™®é€‚æ€§å¢å¼º Prompt
    # å…¬å¼æ ¼å¼è¦æ±‚ï¼ˆæ‰€æœ‰æ¨¡å¼é€šç”¨ï¼‰
    math_format_instruction = """
ã€æ•°å­¦å…¬å¼æ ¼å¼è¦æ±‚ã€‘
- è¡Œå†…å…¬å¼å¿…é¡»ç”¨å•ä¸ªç¾å…ƒç¬¦å·åŒ…è£¹ï¼Œå¦‚ï¼š$a^2 + b^2 = c^2$
- å—çº§å…¬å¼å¿…é¡»ç”¨åŒç¾å…ƒç¬¦å·åŒ…è£¹ï¼Œå¦‚ï¼š$$\\frac{\\partial u}{\\partial t} = 0$$
- ç¦æ­¢ä½¿ç”¨ \\( \\) æˆ– \\[ \\] æ ¼å¼
- æ‰€æœ‰å¸Œè…Šå­—æ¯å¦‚ $\\varphi$, $\\alpha$, $\\partial$ ç­‰å¿…é¡»ç”¨ $ åŒ…è£¹
"""
    
    query_suffix = ""
    if "åˆå­¦è€…" in level:
        query_suffix = f"""\n\nã€æŒ‡ä»¤ï¼šç›´è§‰ç§‘æ™®æ¨¡å¼ã€‘
        1. ğŸš« ä¸¥ç¦ä½¿ç”¨æ™¦æ¶©ä¸“ä¸šæœ¯è¯­ï¼Œå¿…é¡»ç”¨å¤§ç™½è¯ã€‚
        2. âœ… æ ¸å¿ƒï¼šä½¿ç”¨ç”Ÿæ´»ä¸­çš„ç±»æ¯”ï¼ˆå¦‚æŠŠç”µè·¯æ¯”ä½œæ°´ç®¡ï¼‰ã€‚
        3. è¯­æ°”ï¼šå¹½é»˜é£è¶£çš„ç§‘æ™®åšä¸»ã€‚
        {math_format_instruction}
        """
    elif "ä¸“å®¶" in level:
        query_suffix = f"""\n\nã€æŒ‡ä»¤ï¼šæ·±åº¦ç ”è®¨æ¨¡å¼ã€‘
        1. âš ï¸ è·³è¿‡åŸºç¡€å®šä¹‰ï¼Œå‡è®¾ç”¨æˆ·æ˜¯åŒè¡Œã€‚
        2. âœ… æ ¸å¿ƒï¼šåˆ‡å…¥é—®é¢˜æœ¬è´¨ã€åº•å±‚æœºåˆ¶ã€å±€é™æ€§ã€‚
        3. è¯­æ°”ï¼šæåº¦ç®€ç»ƒã€å­¦æœ¯ã€é«˜å†·ã€‚
        {math_format_instruction}
        """
    else:
        query_suffix = f"""\n\nã€æŒ‡ä»¤ï¼šæ ‡å‡†æ•™å­¦æ¨¡å¼ã€‘
        1. ç›®æ ‡ï¼šå¸®åŠ©é€šè¿‡æœŸæœ«è€ƒè¯•ã€‚
        2. âœ… ç»“æ„ï¼šå®šä¹‰ -> å…¬å¼ -> ç‰©ç†æ„ä¹‰ -> è€ƒç‚¹ã€‚
        3. è¯­æ°”ï¼šè€å¿ƒçš„å¤§å­¦åŠ©æ•™ã€‚
        {math_format_instruction}
        """

    # DeepSeek æ–‡æœ¬æ¨¡å‹è°ƒç”¨
    async def safe_deepseek_call(prompt, system_prompt="You are a helpful AI tutor.", history_messages=[], **kwargs):
        # DeepSeek ä¸æ”¯æŒ response_format å’Œ keyword_extraction åŠŸèƒ½
        kwargs.pop('response_format', None)
        kwargs.pop('keyword_extraction', None)
        
        # æ¸…æ´—å›¾ç‰‡æ¶ˆæ¯ (DeepSeek ä¸æ”¯æŒå›¾ç‰‡)
        if "messages" in kwargs:
            clean_msgs = []
            for msg in kwargs["messages"]:
                content = msg.get("content")
                if isinstance(content, list):
                    text_content = "".join([item.get("text", "") for item in content if isinstance(item, dict) and item.get("type") == "text"])
                    clean_msgs.append({"role": msg["role"], "content": text_content})
                else:
                    clean_msgs.append(msg)
            kwargs["messages"] = clean_msgs

        response = await openai_complete_if_cache(
            "deepseek-chat", prompt, system_prompt=system_prompt, 
            history_messages=history_messages, api_key=api_key, base_url=base_url, **kwargs
        )
        raw_text = response.replace("```json", "").replace("```", "").strip() if isinstance(response, str) else str(response)
        return process_math_format(raw_text)

    # è§†è§‰æ¨¡å‹è°ƒç”¨ (æ”¯æŒå›¾åƒå¤„ç†)
    # ä¸åŒæ¨¡å‹çš„å›¾ç‰‡æ•°é‡é™åˆ¶ï¼š
    #   - æ™ºè°± GLM-4V: 1 å¼ 
    #   - é˜¿é‡Œ Qwen-VL-Max: çº¦ 10 å¼ 
    #   - ç¡…åŸºæµåŠ¨ Qwen2-VL: çº¦ 10 å¼ 
    vision_provider = os.getenv("VISION_PROVIDER", "zhipu").lower()
    if vision_provider == "qwen":
        MAX_IMAGES_PER_REQUEST = 10  # é˜¿é‡Œ Qwen-VL-Max æ”¯æŒå¤šå›¾
    elif vision_provider == "siliconflow":
        MAX_IMAGES_PER_REQUEST = 10  # ç¡…åŸºæµåŠ¨ä¹Ÿæ”¯æŒå¤šå›¾
    else:
        MAX_IMAGES_PER_REQUEST = 1   # æ™ºè°± GLM-4V é™åˆ¶ä¸º 1 å¼ 
    
    # è¾…åŠ©å‡½æ•°ï¼šä» messages æå–çº¯æ–‡æœ¬å’Œå›¾ç‰‡
    def extract_content_from_messages(messages):
        """ä» messages ä¸­åˆ†ç¦»æ–‡æœ¬å’Œå›¾ç‰‡"""
        text_parts = []
        images = []
        system_content = None
        
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content")
            
            if role == "system":
                system_content = content if isinstance(content, str) else ""
                continue
                
            if isinstance(content, list):
                for item in content:
                    if item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                    elif item.get("type") == "image_url":
                        images.append(item)
            elif isinstance(content, str):
                text_parts.append(content)
                
        return system_content, "\n".join(text_parts), images
    
    # è¾…åŠ©å‡½æ•°ï¼šæ„å»ºå•æ‰¹æ¬¡çš„ VLM æ¶ˆæ¯
    def build_batch_messages(system_prompt, text_content, batch_images, batch_num=None, total_batches=None):
        """æ„å»ºå•æ‰¹æ¬¡çš„ VLM æ¶ˆæ¯æ ¼å¼"""
        content_parts = []
        
        # æ·»åŠ æ–‡æœ¬å†…å®¹
        if batch_num and total_batches and total_batches > 1:
            batch_info = f"\n\n[è¿™æ˜¯ç¬¬ {batch_num}/{total_batches} æ‰¹å›¾ç‰‡åˆ†æ]"
            content_parts.append({"type": "text", "text": text_content + batch_info})
        else:
            content_parts.append({"type": "text", "text": text_content})
        
        # æ·»åŠ å›¾ç‰‡
        for img in batch_images:
            content_parts.append(img)
        
        msgs = []
        if system_prompt:
            msgs.append({"role": "system", "content": system_prompt})
        msgs.append({"role": "user", "content": content_parts})
        
        return msgs
    
    async def vision_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # VLM ä¸æ”¯æŒ response_format å’Œ keyword_extraction åŠŸèƒ½
        kwargs.pop('response_format', None)
        kwargs.pop('keyword_extraction', None)
        
        # å¦‚æœæ²¡æœ‰é…ç½®è§†è§‰ API Keyï¼Œå›é€€åˆ° DeepSeek (ä»…æ–‡æœ¬)
        if not vision_api_key:
            print("âš ï¸ æœªé…ç½®è§†è§‰æ¨¡å‹ API Keyï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼")
            return await safe_deepseek_call(prompt, system_prompt, history_messages, **kwargs)
        
        # å¦‚æœæä¾›äº† messages æ ¼å¼ (å¤šæ¨¡æ€ VLM å¢å¼ºæŸ¥è¯¢)
        if messages:
            # æå–ç³»ç»Ÿæç¤ºã€æ–‡æœ¬å†…å®¹å’Œæ‰€æœ‰å›¾ç‰‡
            sys_prompt, text_content, all_images = extract_content_from_messages(messages)
            
            if not all_images:
                # æ²¡æœ‰å›¾ç‰‡ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼
                return await safe_deepseek_call(text_content or prompt, sys_prompt or system_prompt, history_messages, **kwargs)
            
            total_images = len(all_images)
            print(f"ğŸ“· æ£€æµ‹åˆ° {total_images} å¼ å›¾ç‰‡ï¼Œæ¯æ‰¹æœ€å¤š {MAX_IMAGES_PER_REQUEST} å¼ ")
            print(f"ğŸ”§ ä½¿ç”¨è§†è§‰æ¨¡å‹: {vision_model} @ {vision_base_url}")
            
            # å¦‚æœå›¾ç‰‡æ•°é‡åœ¨é™åˆ¶å†…ï¼Œç›´æ¥å¤„ç†
            if total_images <= MAX_IMAGES_PER_REQUEST:
                batch_messages = build_batch_messages(sys_prompt or system_prompt, text_content, all_images)
                try:
                    print(f"ğŸš€ æ­£åœ¨è°ƒç”¨ VLM: {vision_model}...")
                    response = await openai_complete_if_cache(
                        vision_model, "",
                        system_prompt=None, history_messages=[],
                        messages=batch_messages,
                        api_key=vision_api_key, base_url=vision_base_url, **kwargs
                    )
                    print(f"âœ… VLM è°ƒç”¨æˆåŠŸï¼å“åº”é•¿åº¦: {len(str(response))} å­—ç¬¦")
                    raw_text = response.replace("```json", "").replace("```", "").strip() if isinstance(response, str) else str(response)
                    return process_math_format(raw_text)
                except Exception as e:
                    print(f"âŒ VLM è°ƒç”¨å¤±è´¥: {e}")
                    print(f"âš ï¸ å›é€€åˆ° DeepSeek çº¯æ–‡æœ¬æ¨¡å¼")
                    return await safe_deepseek_call(text_content or prompt, sys_prompt or system_prompt, history_messages, **kwargs)
            
            # === å›¾ç‰‡åˆ†æ‰¹å¤„ç† ===
            # å°†å›¾ç‰‡åˆ†æˆå¤šä¸ªæ‰¹æ¬¡
            batches = []
            for i in range(0, total_images, MAX_IMAGES_PER_REQUEST):
                batch = all_images[i:i + MAX_IMAGES_PER_REQUEST]
                batches.append(batch)
            
            total_batches = len(batches)
            print(f"ğŸ“¦ å°† {total_images} å¼ å›¾ç‰‡åˆ†æˆ {total_batches} æ‰¹å¤„ç†")
            
            # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
            batch_results = []
            for batch_idx, batch_images in enumerate(batches, 1):
                print(f"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {batch_idx}/{total_batches} æ‰¹ ({len(batch_images)} å¼ å›¾ç‰‡)...")
                
                batch_messages = build_batch_messages(
                    sys_prompt or system_prompt, 
                    text_content, 
                    batch_images,
                    batch_num=batch_idx,
                    total_batches=total_batches
                )
                
                try:
                    response = await openai_complete_if_cache(
                        vision_model, "",
                        system_prompt=None, history_messages=[],
                        messages=batch_messages,
                        api_key=vision_api_key, base_url=vision_base_url, **kwargs
                    )
                    result = response.replace("```json", "").replace("```", "").strip() if isinstance(response, str) else str(response)
                    batch_results.append(f"ã€ç¬¬ {batch_idx} æ‰¹å›¾ç‰‡åˆ†æã€‘\n{result}")
                    print(f"âœ… ç¬¬ {batch_idx} æ‰¹å¤„ç†å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {batch_idx} æ‰¹ VLM è°ƒç”¨å¤±è´¥: {e}")
                    batch_results.append(f"ã€ç¬¬ {batch_idx} æ‰¹å›¾ç‰‡åˆ†æå¤±è´¥ã€‘")
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ‰¹æ¬¡æˆåŠŸï¼Œç›´æ¥è¿”å›
            if len(batch_results) == 1:
                return process_math_format(batch_results[0])
            
            # ä½¿ç”¨ DeepSeek ç»¼åˆæ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
            print("ğŸ§  æ­£åœ¨ç”¨ DeepSeek ç»¼åˆæ‰€æœ‰æ‰¹æ¬¡çš„åˆ†æç»“æœ...")
            combined_prompt = f"""è¯·ç»¼åˆä»¥ä¸‹å¤šæ‰¹æ¬¡çš„å›¾ç‰‡åˆ†æç»“æœï¼Œç»™å‡ºå®Œæ•´ã€è¿è´¯çš„å›ç­”ï¼š

{chr(10).join(batch_results)}

---
åŸå§‹é—®é¢˜ä¸Šä¸‹æ–‡ï¼š
{text_content[:2000]}...

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰æ‰¹æ¬¡çš„åˆ†æï¼Œç»™å‡ºç»Ÿä¸€ã€å®Œæ•´çš„å›ç­”ã€‚"""
            
            final_response = await safe_deepseek_call(
                combined_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æåŠ©æ‰‹ï¼Œè¯·ç»¼åˆå¤šä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡åˆ†æç»“æœï¼Œç»™å‡ºå®Œæ•´è¿è´¯çš„å›ç­”ã€‚",
                history_messages=[]
            )
            return process_math_format(final_response)
                
        # å¦‚æœæä¾›äº†å•å¼ å›¾ç‰‡
        elif image_data:
            built_messages = []
            if system_prompt:
                built_messages.append({"role": "system", "content": system_prompt})
            built_messages.append({
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]
            })
            try:
                response = await openai_complete_if_cache(
                    vision_model, "",
                    system_prompt=None, history_messages=[],
                    messages=built_messages,
                    api_key=vision_api_key, base_url=vision_base_url, **kwargs
                )
            except Exception as e:
                print(f"âš ï¸ VLM è°ƒç”¨å¤±è´¥: {e}ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼")
                return await safe_deepseek_call(prompt, system_prompt, history_messages, **kwargs)
        # çº¯æ–‡æœ¬ï¼Œä½¿ç”¨ DeepSeek
        else:
            return await safe_deepseek_call(prompt, system_prompt, history_messages, **kwargs)
        
        raw_text = response.replace("```json", "").replace("```", "").strip() if isinstance(response, str) else str(response)
        return process_math_format(raw_text)

    rag = RAGAnything(
        config=RAGAnythingConfig(working_dir="./rag_storage", parser="mineru", parse_method="auto"),
        llm_model_func=safe_deepseek_call,
        vision_model_func=vision_func,
        embedding_func=embedding_func
    )

    if file_path:
        await rag.process_document_complete(file_path=file_path, output_dir="./output", parse_method="auto")

    return await rag.aquery(query + query_suffix, mode="hybrid")

# === 7. ç•Œé¢ UI æ„å»º ===
with st.sidebar:
    st.image("https://img.icons8.com/color/96/artificial-intelligence.png", width=60)
    st.title("âš™ï¸ å­¦ä¹ è®¾ç½®")
    user_level = st.radio("æˆ‘æ˜¯è°ï¼Ÿ", ["ğŸ‘¶ åˆå­¦è€… (é€šä¿—æ˜“æ‡‚)", "ğŸ‘¨â€ğŸ“ æœ¬ç§‘ç”Ÿ (ä¸“ä¸šæ¨å¯¼)", "ğŸ‘¨â€ğŸ”¬ é¢†åŸŸä¸“å®¶ (æ·±åº¦ç ”è®¨)"], index=1)
    st.divider()
    
    st.header("ğŸ“‚ çŸ¥è¯†åº“")
    uploaded_file = st.file_uploader("ä¸Šä¼ æ•™æ (PDF)", type=["pdf"])
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“ç¼“å­˜"):
        import shutil
        if os.path.exists("./rag_storage"): shutil.rmtree("./rag_storage")
        if os.path.exists("./output"): shutil.rmtree("./output")
        st.success("ç¼“å­˜å·²æ¸…ç©ºï¼è¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶ã€‚")

# === å…³é”®ä¿®å¤ï¼šåœ¨è¿™é‡Œå…¨å±€å¤„ç† file_pathï¼Œç¡®ä¿æ— è®ºæ€ä¹ˆè§¦å‘éƒ½èƒ½æ‹¿åˆ°è·¯å¾„ ===
file_path = None
if uploaded_file:
    os.makedirs("uploads", exist_ok=True)
    file_path = os.path.join("uploads", uploaded_file.name)
    # é¿å…é‡å¤å†™å…¥
    if not os.path.exists(file_path):
        with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
elif os.path.exists("uploads") and len(os.listdir("uploads")) > 0:
    # å¦‚æœæ²¡é‡æ–°ä¸Šä¼ ï¼Œä½†æ–‡ä»¶å¤¹é‡Œæœ‰æ—§æ–‡ä»¶ï¼Œä¹Ÿè‡ªåŠ¨è¯»å–
    file_path = os.path.join("uploads", os.listdir("uploads")[0])

# === ä¸»ç•Œé¢å†…å®¹ ===
st.title("ğŸ“ æ–°å·¥ç§‘ AI åŠ©æ•™ç³»ç»Ÿ")
# è·å–è§†è§‰æ¨¡å‹ä¿¡æ¯æ˜¾ç¤º
_vision_provider = os.getenv("VISION_PROVIDER", "zhipu")
_vision_model_name = VISION_PROVIDERS.get(_vision_provider, {}).get("model", "æœªé…ç½®")
if os.getenv("VISION_API_KEY") or os.getenv(VISION_PROVIDERS.get(_vision_provider, {}).get("env_key", "")):
    vision_status = f"ğŸ–¼ï¸ {_vision_model_name}"
else:
    vision_status = "ğŸ–¼ï¸ æœªé…ç½®"
st.caption(f"å½“å‰æ¨¡å¼ï¼š{user_level} | æ–‡æœ¬å¼•æ“ï¼šDeepSeek | è§†è§‰å¼•æ“ï¼š{vision_status} | Embeddingï¼šBGE-Small")

if "messages" not in st.session_state: st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

# --- æŒ‰é’®å›è°ƒå‡½æ•° ---
def click_quiz_btn():
    st.session_state.messages.append({
        "role": "user", 
        "content": "è¯·æ ¹æ®å½“å‰æ–‡æ¡£å†…å®¹ï¼Œå‡º 3 é“å•é¡¹é€‰æ‹©é¢˜ï¼Œè€ƒå¯Ÿæ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶é™„å¸¦ç­”æ¡ˆè§£æã€‚"
    })

col1, col2 = st.columns(2)
with col1:
    # ç»‘å®šå›è°ƒ
    st.button("ğŸ“ ç”Ÿæˆéšå ‚æµ‹éªŒ (3é¢˜)", on_click=click_quiz_btn)

# å¤„ç†è¾“å…¥æ¡†
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆæ”¯æŒä¸“ä¸šå…¬å¼è¯¢é—®ï¼‰..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.rerun()

# --- ç»Ÿä¸€åº”ç­”é€»è¾‘ (æ ¸å¿ƒä¿®å¤ç‚¹) ---
# åªè¦æœ€æ–°ä¸€æ¡æ¶ˆæ¯æ˜¯ç”¨æˆ·å‘çš„ï¼Œå°±å¼€å§‹å¤„ç†
if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    last_user_query = st.session_state.messages[-1]["content"]
    
    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  DeepSeek æ­£åœ¨æ€è€ƒ..."):
            try:
                # æ£€æŸ¥ file_path æ˜¯å¦æœ‰æ•ˆ (è¿™é‡Œ file_path æ˜¯å…¨å±€å˜é‡ï¼Œè‚¯å®šèƒ½è®¿é—®åˆ°)
                if not file_path and not os.path.exists("./rag_storage"):
                    error_msg = "è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF æ•™æï¼"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                else:
                    loop = st.session_state.loop
                    # è°ƒç”¨ RAG
                    response = loop.run_until_complete(run_rag(file_path, last_user_query, user_level))
                    
                    import json
                    try:
                        if isinstance(response, str):
                            final_ans = json.loads(response).get("answer", response)
                        else:
                            final_ans = str(response)
                    except:
                        final_ans = str(response)
                    
                    final_ans = process_math_format(final_ans)
                    
                    st.markdown(final_ans)
                    st.session_state.messages.append({"role": "assistant", "content": final_ans})
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"Error: {e}"})