import streamlit as st
import sys
import os
import asyncio
import re
from pathlib import Path

# === 1. Windows å¼‚æ­¥ç­–ç•¥è¡¥ä¸ ===
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# === 2. åŸºç¡€ç¯å¢ƒé…ç½® ===
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
sys.path.append(os.getcwd())

from sentence_transformers import SentenceTransformer
from lightrag.llm.openai import openai_complete_if_cache
from lightrag.utils import EmbeddingFunc
from raganything import RAGAnything, RAGAnythingConfig
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env", override=False)

st.set_page_config(page_title="æ–°å·¥ç§‘ AI åŠ©æ•™", layout="wide", page_icon="ğŸ“")

# === 3. æ°¸ä¹…äº‹ä»¶å¾ªç¯ç®¡ç† ===
if "loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    st.session_state.loop = loop
else:
    asyncio.set_event_loop(st.session_state.loop)

# === 4. è¾…åŠ©å‡½æ•°ï¼šæ¸…æ´—æ•°å­¦å…¬å¼æ ¼å¼ ===
def process_math_format(text):
    if not isinstance(text, str): return str(text)
    text = re.sub(r'\\\((.*?)\\\)', r'$\1$', text, flags=re.DOTALL)
    text = re.sub(r'\\\[(.*?)\\\]', r'$$\1$$', text, flags=re.DOTALL)
    def remove_code_ticks(match):
        content = match.group(1)
        if '\\' in content or '=' in content or '^' in content:
            return f"${content.strip('$')}$"
        return match.group(0)
    text = re.sub(r'`(.*?)`', remove_code_ticks, text)
    return text

# === 5. æ¨¡å‹åŠ è½½ ===
@st.cache_resource
def load_local_model_only():
    print("æ­£åœ¨åŠ è½½æœ¬åœ° BGE-Small ä¸­æ–‡æ¨¡å‹...")
    return SentenceTransformer('BAAI/bge-small-zh-v1.5')

# === 6. æ ¸å¿ƒ RAG ä¸šåŠ¡é€»è¾‘ ===
async def run_rag(file_path, query, level):
    api_key = os.getenv("LLM_BINDING_API_KEY")
    base_url = os.getenv("LLM_BINDING_HOST")
    
    local_model = load_local_model_only()

    async def _current_loop_embed(texts):
        return await asyncio.to_thread(lambda: local_model.encode(texts))

    embedding_func = EmbeddingFunc(
        embedding_dim=512, 
        max_token_size=512, 
        func=_current_loop_embed
    )

    # æ™®é€‚æ€§å¢å¼º Prompt
    query_suffix = ""
    if "åˆå­¦è€…" in level:
        query_suffix = """\n\nã€æŒ‡ä»¤ï¼šç›´è§‰ç§‘æ™®æ¨¡å¼ã€‘
        1. ğŸš« ä¸¥ç¦ä½¿ç”¨æ™¦æ¶©ä¸“ä¸šæœ¯è¯­ï¼Œå¿…é¡»ç”¨å¤§ç™½è¯ã€‚
        2. âœ… æ ¸å¿ƒï¼šä½¿ç”¨ç”Ÿæ´»ä¸­çš„ç±»æ¯”ï¼ˆå¦‚æŠŠç”µè·¯æ¯”ä½œæ°´ç®¡ï¼‰ã€‚
        3. è¯­æ°”ï¼šå¹½é»˜é£è¶£çš„ç§‘æ™®åšä¸»ã€‚
        """
    elif "ä¸“å®¶" in level:
        query_suffix = """\n\nã€æŒ‡ä»¤ï¼šæ·±åº¦ç ”è®¨æ¨¡å¼ã€‘
        1. âš ï¸ è·³è¿‡åŸºç¡€å®šä¹‰ï¼Œå‡è®¾ç”¨æˆ·æ˜¯åŒè¡Œã€‚
        2. âœ… æ ¸å¿ƒï¼šåˆ‡å…¥é—®é¢˜æœ¬è´¨ã€åº•å±‚æœºåˆ¶ã€å±€é™æ€§ã€‚
        3. è¯­æ°”ï¼šæåº¦ç®€ç»ƒã€å­¦æœ¯ã€é«˜å†·ã€‚
        """
    else:
        query_suffix = """\n\nã€æŒ‡ä»¤ï¼šæ ‡å‡†æ•™å­¦æ¨¡å¼ã€‘
        1. ç›®æ ‡ï¼šå¸®åŠ©é€šè¿‡æœŸæœ«è€ƒè¯•ã€‚
        2. âœ… ç»“æ„ï¼šå®šä¹‰ -> å…¬å¼ -> ç‰©ç†æ„ä¹‰ -> è€ƒç‚¹ã€‚
        3. è¯­æ°”ï¼šè€å¿ƒçš„å¤§å­¦åŠ©æ•™ã€‚
        """

    async def safe_deepseek_call(prompt, system_prompt="You are a helpful AI tutor.", history_messages=[], **kwargs):
        if 'response_format' in kwargs: del kwargs['response_format']
        
        # æ¸…æ´—å›¾ç‰‡æ¶ˆæ¯
        if "messages" in kwargs:
            clean_msgs = []
            for msg in kwargs["messages"]:
                content = msg.get("content")
                if isinstance(content, list):
                    text_content = "".join([item.get("text", "") for item in content if isinstance(item, dict) and item.get("type") == "text"])
                    clean_msgs.append({"role": msg["role"], "content": text_content})
                else:
                    clean_msgs.append(msg)
            kwargs["messages"] = clean_msgs

        response = await openai_complete_if_cache(
            "deepseek-chat", prompt, system_prompt=system_prompt, 
            history_messages=history_messages, api_key=api_key, base_url=base_url, **kwargs
        )
        raw_text = response.replace("```json", "").replace("```", "").strip() if isinstance(response, str) else str(response)
        return process_math_format(raw_text)

    async def vision_func(prompt, **kwargs): return await safe_deepseek_call(prompt, **kwargs)

    rag = RAGAnything(
        config=RAGAnythingConfig(working_dir="./rag_storage", parser="mineru", parse_method="auto"),
        llm_model_func=safe_deepseek_call,
        vision_model_func=vision_func,
        embedding_func=embedding_func
    )

    if file_path:
        await rag.process_document_complete(file_path=file_path, output_dir="./output", parse_method="auto")

    return await rag.aquery(query + query_suffix, mode="hybrid")

# === 7. ç•Œé¢ UI æ„å»º ===
with st.sidebar:
    st.image("https://img.icons8.com/color/96/artificial-intelligence.png", width=60)
    st.title("âš™ï¸ å­¦ä¹ è®¾ç½®")
    user_level = st.radio("æˆ‘æ˜¯è°ï¼Ÿ", ["ğŸ‘¶ åˆå­¦è€… (é€šä¿—æ˜“æ‡‚)", "ğŸ‘¨â€ğŸ“ æœ¬ç§‘ç”Ÿ (ä¸“ä¸šæ¨å¯¼)", "ğŸ‘¨â€ğŸ”¬ é¢†åŸŸä¸“å®¶ (æ·±åº¦ç ”è®¨)"], index=1)
    st.divider()
    
    st.header("ğŸ“‚ çŸ¥è¯†åº“")
    uploaded_file = st.file_uploader("ä¸Šä¼ æ•™æ (PDF)", type=["pdf"])
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“ç¼“å­˜"):
        import shutil
        if os.path.exists("./rag_storage"): shutil.rmtree("./rag_storage")
        if os.path.exists("./output"): shutil.rmtree("./output")
        st.success("ç¼“å­˜å·²æ¸…ç©ºï¼è¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶ã€‚")

# === å…³é”®ä¿®å¤ï¼šåœ¨è¿™é‡Œå…¨å±€å¤„ç† file_pathï¼Œç¡®ä¿æ— è®ºæ€ä¹ˆè§¦å‘éƒ½èƒ½æ‹¿åˆ°è·¯å¾„ ===
file_path = None
if uploaded_file:
    os.makedirs("uploads", exist_ok=True)
    file_path = os.path.join("uploads", uploaded_file.name)
    # é¿å…é‡å¤å†™å…¥
    if not os.path.exists(file_path):
        with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
elif os.path.exists("uploads") and len(os.listdir("uploads")) > 0:
    # å¦‚æœæ²¡é‡æ–°ä¸Šä¼ ï¼Œä½†æ–‡ä»¶å¤¹é‡Œæœ‰æ—§æ–‡ä»¶ï¼Œä¹Ÿè‡ªåŠ¨è¯»å–
    file_path = os.path.join("uploads", os.listdir("uploads")[0])

# === ä¸»ç•Œé¢å†…å®¹ ===
st.title("ğŸ“ æ–°å·¥ç§‘ AI åŠ©æ•™ç³»ç»Ÿ")
st.caption(f"å½“å‰æ¨¡å¼ï¼š{user_level} | å¼•æ“ï¼šDeepSeek-V3 + BGE-Small")

if "messages" not in st.session_state: st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

# --- æŒ‰é’®å›è°ƒå‡½æ•° ---
def click_quiz_btn():
    st.session_state.messages.append({
        "role": "user", 
        "content": "è¯·æ ¹æ®å½“å‰æ–‡æ¡£å†…å®¹ï¼Œå‡º 3 é“å•é¡¹é€‰æ‹©é¢˜ï¼Œè€ƒå¯Ÿæ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶é™„å¸¦ç­”æ¡ˆè§£æã€‚"
    })

col1, col2 = st.columns(2)
with col1:
    # ç»‘å®šå›è°ƒ
    st.button("ğŸ“ ç”Ÿæˆéšå ‚æµ‹éªŒ (3é¢˜)", on_click=click_quiz_btn)

# å¤„ç†è¾“å…¥æ¡†
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆæ”¯æŒä¸“ä¸šå…¬å¼è¯¢é—®ï¼‰..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.rerun()

# --- ç»Ÿä¸€åº”ç­”é€»è¾‘ (æ ¸å¿ƒä¿®å¤ç‚¹) ---
# åªè¦æœ€æ–°ä¸€æ¡æ¶ˆæ¯æ˜¯ç”¨æˆ·å‘çš„ï¼Œå°±å¼€å§‹å¤„ç†
if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    last_user_query = st.session_state.messages[-1]["content"]
    
    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  DeepSeek æ­£åœ¨æ€è€ƒ..."):
            try:
                # æ£€æŸ¥ file_path æ˜¯å¦æœ‰æ•ˆ (è¿™é‡Œ file_path æ˜¯å…¨å±€å˜é‡ï¼Œè‚¯å®šèƒ½è®¿é—®åˆ°)
                if not file_path and not os.path.exists("./rag_storage"):
                    error_msg = "è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF æ•™æï¼"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                else:
                    loop = st.session_state.loop
                    # è°ƒç”¨ RAG
                    response = loop.run_until_complete(run_rag(file_path, last_user_query, user_level))
                    
                    import json
                    try:
                        if isinstance(response, str):
                            final_ans = json.loads(response).get("answer", response)
                        else:
                            final_ans = str(response)
                    except:
                        final_ans = str(response)
                    
                    final_ans = process_math_format(final_ans)
                    
                    st.markdown(final_ans)
                    st.session_state.messages.append({"role": "assistant", "content": final_ans})
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"Error: {e}"})